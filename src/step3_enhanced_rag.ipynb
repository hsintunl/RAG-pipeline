{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d053525-4e8d-437c-870d-621fee0266c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Enhanced RAG (Step 5): Query Rewriting → Retrieval → Reranking → Generation\n",
    "# Baseline choices aligned with Step 3/4 findings:\n",
    "#   - Prompt: Persona\n",
    "#   - Embedding: all-mpnet-base-v2 (dim768)\n",
    "#   - K (final context size): 5\n",
    "#   - Two Enhancements: Query Rewriting + Reranking (Cross-Encoder)\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde1db87-1858-4fd8-8ae3-d2b1dc2a96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fd7a84e-f5b1-4872-9deb-7977824b44b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.30.1 at schema.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.30.1 at common.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.30.1 at milvus.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.30.1 at rg.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.30.1 at feder.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.27.2 is exactly one major version older than the runtime version 6.30.1 at msg.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import pipeline\n",
    "from pymilvus import MilvusClient, FieldSchema, CollectionSchema, DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7643c791-fcc6-4e80-913b-3be486969e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ranking\n",
    "from sentence_transformers import CrossEncoder \n",
    "from typing import List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "941529ad-c5fd-4d1b-b090-9d5ed6186881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading QA file from: /Users/connie/Desktop/Fall 2025/LLM/Assignment2/data/evaluation/all_experiment_generations.csv\n",
      "Passages file: /Users/connie/Desktop/Fall 2025/LLM/Assignment2/data/processed/rag_mini_wiki.csv\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "INPUT_QA_PATH   = \"../data/evaluation/all_experiment_generations.csv\"  # has question & answer\n",
    "PASSAGES_PATH   = \"../data/processed/rag_mini_wiki.csv\"                 # passages to index\n",
    "\n",
    "print(\"Reading QA file from:\", os.path.abspath(INPUT_QA_PATH))\n",
    "print(\"Passages file:\", os.path.abspath(PASSAGES_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674870fe-0fa5-42c7-9ebe-2e5c6dd1946e",
   "metadata": {},
   "source": [
    "## Load QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2739b844-f200-437c-8db3-b58afc03ec72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA shape: (120, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What did James Monroe make in 1817?</td>\n",
       "      <td>two long tours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are Gray Wolves native to North America?</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is English the official language?</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How long is the elephant's gestation period?</td>\n",
       "      <td>22 months</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are diving ducks heavier tha dabbling ducks?</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question          answer\n",
       "0           What did James Monroe make in 1817?  two long tours\n",
       "1      Are Gray Wolves native to North America?             Yes\n",
       "2             Is English the official language?             yes\n",
       "3  How long is the elephant's gestation period?       22 months\n",
       "4  Are diving ducks heavier tha dabbling ducks?             Yes"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df = pd.read_csv(INPUT_QA_PATH)\n",
    "\n",
    "# Keep only what we need for enhanced generation\n",
    "qa_df = qa_df[[\"question\", \"answer\"]].copy()\n",
    "print(\"QA shape:\", qa_df.shape)\n",
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bc3f5b-8e7b-4483-b921-bb9b0e4a6287",
   "metadata": {},
   "source": [
    "## Models & Milvus Client (new DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2b791f6-83f8-41a3-990e-567efa9ae03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milvus DB: /Users/connie/Desktop/Fall 2025/LLM/Assignment2/src/rag_wikipedia_mini_enhanced.db\n"
     ]
    }
   ],
   "source": [
    "# Query Rewriting model\n",
    "query_rewriter = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "# Embedding model\n",
    "EMBEDDING_NAME = \"all-mpnet-base-v2\"\n",
    "embedding_model = SentenceTransformer(EMBEDDING_NAME)\n",
    "\n",
    "# Cross-encoder Reranker\n",
    "RERANKER_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "reranker = CrossEncoder(RERANKER_NAME)\n",
    "\n",
    "# Generation model\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "# 使用新的 Milvus Lite DB\n",
    "ENHANCED_DB_NAME = \"rag_wikipedia_mini_enhanced.db\"\n",
    "client = MilvusClient(ENHANCED_DB_NAME)\n",
    "print(\"Milvus DB:\", os.path.abspath(ENHANCED_DB_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f7c30ed-1c24-4598-943b-ff784102a82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Build Milvus] Rebuilding collection with all-mpnet-base-v2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1759172786.717736 11151799 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 3200 passages → dim=768 in 49.9s\n",
      "Entity count: 3200\n",
      "[Build Milvus] Done.\n"
     ]
    }
   ],
   "source": [
    "def rebuild_milvus_index(client: MilvusClient, passages_csv_path: str, embedding_model: SentenceTransformer, model_name_str: str):\n",
    "    \"\"\"\n",
    "    Build a fresh Milvus collection ('rag_mini') inside the NEW enhanced DB\n",
    "    with embeddings computed by the given embedding_model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n[Build Milvus] Rebuilding collection with {model_name_str} ...\")\n",
    "    df_passages = pd.read_csv(passages_csv_path)\n",
    "    assert \"passages\" in df_passages.columns, \"Expected a 'passages' column in passages CSV.\"\n",
    "\n",
    "    # 1) Encode\n",
    "    t0 = time.time()\n",
    "    embeddings = embedding_model.encode(df_passages[\"passages\"].tolist(), batch_size=64)\n",
    "    vector_dim = embeddings.shape[1]\n",
    "    print(f\"Encoded {len(df_passages)} passages → dim={vector_dim} in {time.time()-t0:.1f}s\")\n",
    "\n",
    "    # 2) Define schema\n",
    "    id_field        = FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=False)\n",
    "    passage_field   = FieldSchema(name=\"passage\", dtype=DataType.VARCHAR, max_length=3000)\n",
    "    embedding_field = FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=vector_dim)\n",
    "    schema = CollectionSchema(fields=[id_field, passage_field, embedding_field], description=\"Enhanced passages\")\n",
    "\n",
    "    # 3) Drop & recreate\n",
    "    if client.has_collection(\"rag_mini\"):\n",
    "        client.drop_collection(\"rag_mini\")\n",
    "    client.create_collection(collection_name=\"rag_mini\", schema=schema, consistency_level=\"Strong\")\n",
    "\n",
    "    # 4) Insert\n",
    "    rag_data = [{\"id\": i, \"passage\": df_passages.iloc[i][\"passages\"], \"embedding\": embeddings[i].tolist()}\n",
    "                for i in range(len(df_passages))]\n",
    "    client.insert(collection_name=\"rag_mini\", data=rag_data)\n",
    "\n",
    "    # 5) Index\n",
    "    index_params = MilvusClient.prepare_index_params()\n",
    "    index_params.add_index(\n",
    "        field_name=\"embedding\",\n",
    "        index_type=\"IVF_FLAT\",       \n",
    "        metric_type=\"COSINE\",\n",
    "        params={\"nlist\": 128}\n",
    "    )\n",
    "    client.create_index(\"rag_mini\", index_params=index_params)\n",
    "    client.load_collection(\"rag_mini\")\n",
    "\n",
    "    # 6) Report\n",
    "    stats = client.get_collection_stats(\"rag_mini\")\n",
    "    print(\"Entity count:\", stats[\"row_count\"])\n",
    "    print(\"[Build Milvus] Done.\")\n",
    "\n",
    "# Build enhanced DB once\n",
    "rebuild_milvus_index(client, PASSAGES_PATH, embedding_model, EMBEDDING_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74b4c4c5-d063-478b-9719-7fa275c544e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N = 10   \n",
    "TOP_K = 5    \n",
    "\n",
    "def rewrite_query(query: str) -> str:\n",
    "    \"\"\"Rewrite user query into clearer, retrieval-friendly text.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a query rewriter for retrieval systems.\n",
    "    Task: Rewrite the following question into a clearer, concise form. \n",
    "    Rules:\n",
    "    - Do NOT answer the question.\n",
    "    - Do NOT change the meaning of the question.\n",
    "    - Keep named entities and dates intact.\n",
    "    - Keep the rewritten query in a question format.\n",
    "\n",
    "    Original Question: {query}\n",
    "    Rewritten Query:\n",
    "    \"\"\"\n",
    "    out = query_rewriter(prompt, max_new_tokens=64, do_sample=False)[0][\"generated_text\"]\n",
    "    return out.strip()\n",
    "\n",
    "def retrieve_and_rerank(query: str, orig_query: str, top_n: int = TOP_N, top_k: int = TOP_K):\n",
    "    \"\"\"Dense retrieval with rewritten query + rerank using cross-encoder against original query.\"\"\"\n",
    "    q_emb = embedding_model.encode(query).tolist()\n",
    "    results = client.search(\n",
    "        collection_name=\"rag_mini\",\n",
    "        data=[q_emb],\n",
    "        anns_field=\"embedding\",\n",
    "        search_params={\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}},\n",
    "        limit=top_n,\n",
    "        output_fields=[\"id\", \"passage\"]\n",
    "    )\n",
    "    passages = [hit[\"entity\"][\"passage\"] for hit in results[0]]\n",
    "    if not passages:\n",
    "        return []\n",
    "\n",
    "    # Cross-encoder: rerank passages w.r.t. original query\n",
    "    pairs  = [[orig_query, p] for p in passages]\n",
    "    scores = reranker.predict(pairs)\n",
    "    ranked = sorted(zip(passages, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [p for p, _ in ranked[:top_k]]\n",
    "\n",
    "\n",
    "PERSONA_PROMPT = \"\"\"You are a historical expert known for concise and authoritative answers.\n",
    "Answer ONLY using the provided context. If the context does not contain the answer, say \"Information not available in the context.\"\n",
    "Keep the answer short and factual.\n",
    "\"\"\"\n",
    "\n",
    "def generate_answer_with_persona(orig_query: str, passages: list[str]) -> str:\n",
    "    context = \"\\n---\\n\".join(passages) if passages else \"\"\n",
    "    prompt = f\"\"\"{PERSONA_PROMPT}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {orig_query}\n",
    "\"\"\"\n",
    "    out = generator(prompt, max_new_tokens=200, do_sample=False)\n",
    "    return out[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "957ad4f1-c1f1-48e7-995a-693dd31b5848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG:   0%|                                     | 0/120 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (726 > 512). Running this sequence through the model will result in indexing errors\n",
      "Enhanced RAG: 100%|███████████████████████████| 120/120 [04:28<00:00,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 268.5s for 120 queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "enhanced_answers   = []\n",
    "rewritten_queries  = []\n",
    "used_passage_sizes = []\n",
    "contexts_joined    = []   \n",
    "\n",
    "t0 = time.time()\n",
    "for i in tqdm(range(len(qa_df)), desc=\"Enhanced RAG\"):\n",
    "    orig_q = qa_df.iloc[i][\"question\"]\n",
    "\n",
    "    # 1) Query Rewriting\n",
    "    rew_q = rewrite_query(orig_q)\n",
    "    rewritten_queries.append(rew_q)\n",
    "\n",
    "    # 2) Retrieval + Reranking\n",
    "    top_passages = retrieve_and_rerank(rew_q, orig_q, top_n=TOP_N, top_k=TOP_K)\n",
    "    used_passage_sizes.append(len(top_passages))\n",
    "    contexts_joined.append(\"\\n---\\n\".join(top_passages))  # 🔥 把 top-k passages 串接起來\n",
    "\n",
    "    # 3) Persona-based Generation\n",
    "    ans = generate_answer_with_persona(orig_q, top_passages)\n",
    "    enhanced_answers.append(ans)\n",
    "\n",
    "print(f\"Total time: {time.time()-t0:.1f}s for {len(qa_df)} queries\")\n",
    "\n",
    "qa_df[\"rewritten_query\"] = rewritten_queries\n",
    "qa_df[\"gen_enhanced_k5_persona_mpnet\"] = enhanced_answers\n",
    "qa_df[\"ctx_count_used\"] = used_passage_sizes\n",
    "qa_df[\"contexts\"] = contexts_joined "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a59685b9-6e83-4205-b136-5ffdc639041c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced results saved → /Users/connie/Desktop/Fall 2025/LLM/Assignment2/data/evaluation/enhanced_experiment_generations.csv\n"
     ]
    }
   ],
   "source": [
    "out_path = os.path.join(\"../data/evaluation\", \"enhanced_experiment_generations.csv\")\n",
    "qa_df.to_csv(out_path, index=False)\n",
    "print(f\"✅ Enhanced results saved → {os.path.abspath(out_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0214b9-ca94-49e2-8f03-68f5c4feddaa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Enhanced RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "726b8c7b-a703-4886-ab90-03466a6f107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_rag_pipeline(query: str, client: MilvusClient, embedding_model: SentenceTransformer, generator, reranker: CrossEncoder, \n",
    "                          k_retrieval: int, k_rerank: int, strategy_template: str) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Executes the Enhanced RAG workflow (Q.R. -> Multi-Query Retrieval -> Reranking -> Generation).\n",
    "    Returns (generated_answer, final_context_list) for RAGAs evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Query Rewriting\n",
    "    all_queries = generate_rewritten_queries(query, generator)\n",
    "    \n",
    "    # 2. Multi-Query Retrieval (Retrieving a large set for high recall)\n",
    "    all_results = []\n",
    "    \n",
    "    for q in all_queries:\n",
    "        q_emb = embedding_model.encode(q).tolist()\n",
    "        results = client.search(\n",
    "            collection_name=\"rag_mini\",\n",
    "            data=[q_emb],\n",
    "            anns_field=\"embedding\",\n",
    "            search_params={\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}},\n",
    "            limit=k_retrieval, # Use K_RETRIEVAL_PRE_RERANK (e.g., 10)\n",
    "            output_fields=[\"passage\"]\n",
    "        )\n",
    "        passages = [hit[\"entity\"][\"passage\"] for hit in results[0]]\n",
    "        all_results.extend(passages)\n",
    "        \n",
    "    # Remove duplicates from the large set\n",
    "    unique_passages = list(set(all_results)) \n",
    "\n",
    "    # 3. Reranking (Filtering out noise to maintain Faithfulness)\n",
    "    if not unique_passages:\n",
    "        return \"Information not available in the context.\", [\"No context retrieved.\"]\n",
    "        \n",
    "    final_context_list = rerank_passages(query, unique_passages, reranker, k_rerank)\n",
    "\n",
    "    # 4. Context Combination for LLM\n",
    "    context_for_llm = \"\\n---\\n\".join(final_context_list) \n",
    "\n",
    "    # 5. Generation\n",
    "    prompt = strategy_template.format(context=context_for_llm, question=query)\n",
    "    out = generator(prompt, max_new_tokens=200, do_sample=False)\n",
    "    final_answer = out[0][\"generated_text\"]\n",
    "    \n",
    "    return final_answer, final_context_list\n",
    "\n",
    "\n",
    "def run_enhanced_experiment(qa_df_subset: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Runs the Enhanced RAG Pipeline across all subset queries and stores results.\n",
    "    \"\"\"\n",
    "    if qa_df_subset.empty:\n",
    "        print(\"Error: Subset DataFrame is empty. Cannot run experiment.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    print(f\"\\n--- Starting Enhanced RAG Pipeline for {len(qa_df_subset)} Queries (Step 5/6) ---\")\n",
    "    \n",
    "    generated_answers = []\n",
    "    retrieved_contexts = []\n",
    "    strategy_template = PROMPT_NAIVE # Using the fixed strategy (Naive Prompt) for this experiment\n",
    "    \n",
    "    for i in tqdm(range(len(qa_df_subset)), desc=\"Enhanced RAG (Q.R. + Rerank)\"):\n",
    "        q = qa_df_subset.iloc[i][\"question\"]\n",
    "        \n",
    "        # Execute the Enhanced RAG workflow\n",
    "        answer, context_list = enhanced_rag_pipeline(\n",
    "            q, CLIENT, EMBEDDING_MODEL, GENERATOR, RERANKER, \n",
    "            K_RETRIEVAL_PRE_RERANK, K_RERANK_FINAL, strategy_template\n",
    "        )\n",
    "        \n",
    "        generated_answers.append(answer)\n",
    "        retrieved_contexts.append(context_list) # Store context as List[str] for RAGAs\n",
    "\n",
    "    # Prepare results DataFrame\n",
    "    results_df = qa_df_subset.copy()\n",
    "    results_df[\"enhanced_answer\"] = generated_answers\n",
    "    # Store contexts as string representation of a list; will be converted back for RAGAs\n",
    "    results_df[\"enhanced_contexts\"] = [str(ctx) for ctx in retrieved_contexts] \n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d60fd83-07c8-4504-9d39-e1881fa15968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Enhanced RAG Pipeline for 120 Queries (Step 5/6) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced RAG (Q.R. + Rerank):   0%|                     | 0/120 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Enhanced RAG (Q.R. + Rerank):  19%|██▎         | 23/120 [01:13<05:11,  3.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Run the enhanced experiment\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     enhanced_results_df \u001b[38;5;241m=\u001b[39m run_enhanced_experiment(qa_df_subset)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Save the results for Step 6 RAGAs evaluation\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[15], line 65\u001b[0m, in \u001b[0;36mrun_enhanced_experiment\u001b[0;34m(qa_df_subset)\u001b[0m\n\u001b[1;32m     62\u001b[0m q \u001b[38;5;241m=\u001b[39m qa_df_subset\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Execute the Enhanced RAG workflow\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m answer, context_list \u001b[38;5;241m=\u001b[39m enhanced_rag_pipeline(\n\u001b[1;32m     66\u001b[0m     q, CLIENT, EMBEDDING_MODEL, GENERATOR, RERANKER, \n\u001b[1;32m     67\u001b[0m     K_RETRIEVAL_PRE_RERANK, K_RERANK_FINAL, strategy_template\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     70\u001b[0m generated_answers\u001b[38;5;241m.\u001b[39mappend(answer)\n\u001b[1;32m     71\u001b[0m retrieved_contexts\u001b[38;5;241m.\u001b[39mappend(context_list) \u001b[38;5;66;03m# Store context as List[str] for RAGAs\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 41\u001b[0m, in \u001b[0;36menhanced_rag_pipeline\u001b[0;34m(query, client, embedding_model, generator, reranker, k_retrieval, k_rerank, strategy_template)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# 5. Generation\u001b[39;00m\n\u001b[1;32m     40\u001b[0m prompt \u001b[38;5;241m=\u001b[39m strategy_template\u001b[38;5;241m.\u001b[39mformat(context\u001b[38;5;241m=\u001b[39mcontext_for_llm, question\u001b[38;5;241m=\u001b[39mquery)\n\u001b[0;32m---> 41\u001b[0m out \u001b[38;5;241m=\u001b[39m generator(prompt, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     42\u001b[0m final_answer \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_answer, final_context_list\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:173\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[1;32m    178\u001b[0m     ):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:1379\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1372\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1373\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         )\n\u001b[1;32m   1377\u001b[0m     )\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:1386\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1385\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1386\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1387\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:1286\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1285\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1286\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1287\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/text2text_generation.py:202\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    200\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 202\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[1;32m    203\u001b[0m out_b \u001b[38;5;241m=\u001b[39m output_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py:2465\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2458\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2459\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2460\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2461\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2462\u001b[0m     )\n\u001b[1;32m   2464\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2465\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[1;32m   2466\u001b[0m         input_ids,\n\u001b[1;32m   2467\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   2468\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   2469\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   2470\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   2471\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   2472\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2473\u001b[0m     )\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2476\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2477\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2478\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2479\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2480\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2482\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py:3434\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3432\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3434\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3436\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3437\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3438\u001b[0m     outputs,\n\u001b[1;32m   3439\u001b[0m     model_kwargs,\n\u001b[1;32m   3440\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3441\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1905\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1902\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1905\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m   1906\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[1;32m   1907\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask,\n\u001b[1;32m   1908\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39mdecoder_inputs_embeds,\n\u001b[1;32m   1909\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1910\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m   1911\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1912\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mdecoder_head_mask,\n\u001b[1;32m   1913\u001b[0m     cross_attn_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_head_mask,\n\u001b[1;32m   1914\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1915\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1916\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1917\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1918\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1919\u001b[0m )\n\u001b[1;32m   1921\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1131\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1114\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1115\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1116\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         cache_position,\n\u001b[1;32m   1129\u001b[0m     )\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m   1132\u001b[0m         hidden_states,\n\u001b[1;32m   1133\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m   1134\u001b[0m         position_bias\u001b[38;5;241m=\u001b[39mposition_bias,\n\u001b[1;32m   1135\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1136\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m   1137\u001b[0m         encoder_decoder_position_bias\u001b[38;5;241m=\u001b[39mencoder_decoder_position_bias,\n\u001b[1;32m   1138\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[1;32m   1139\u001b[0m         cross_attn_layer_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_layer_head_mask,\n\u001b[1;32m   1140\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1141\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1142\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1143\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1144\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1145\u001b[0m     )\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:682\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    668\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    680\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    681\u001b[0m ):\n\u001b[0;32m--> 682\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer[\u001b[38;5;241m0\u001b[39m](\n\u001b[1;32m    683\u001b[0m         hidden_states,\n\u001b[1;32m    684\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    685\u001b[0m         position_bias\u001b[38;5;241m=\u001b[39mposition_bias,\n\u001b[1;32m    686\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[1;32m    687\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    688\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    689\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    690\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    691\u001b[0m     )\n\u001b[1;32m    692\u001b[0m     hidden_states, past_key_value \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    693\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:600\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    590\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    597\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m ):\n\u001b[1;32m    599\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 600\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSelfAttention(\n\u001b[1;32m    601\u001b[0m         normed_hidden_states,\n\u001b[1;32m    602\u001b[0m         mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    603\u001b[0m         position_bias\u001b[38;5;241m=\u001b[39mposition_bias,\n\u001b[1;32m    604\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[1;32m    605\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    606\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    607\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    608\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    609\u001b[0m     )\n\u001b[1;32m    610\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    611\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:566\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer_head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m*\u001b[39m layer_head_mask\n\u001b[0;32m--> 566\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attn_weights, value_states)\n\u001b[1;32m    568\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    569\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_dim)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Run the enhanced experiment\n",
    "    enhanced_results_df = run_enhanced_experiment(qa_df_subset)\n",
    "    \n",
    "    # Save the results for Step 6 RAGAs evaluation\n",
    "    os.makedirs(\"../data/evaluation\", exist_ok=True)\n",
    "    save_path = \"../data/evaluation/enhanced_rag_generations.csv\"\n",
    "    enhanced_results_df.to_csv(save_path, index=False)\n",
    "    \n",
    "    print(\"\\nEnhanced RAG experiment complete.\")\n",
    "    print(f\"Results saved to {save_path} for Step 6 RAGAs evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a93471-9273-4a9f-95f3-2966c7e885e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
