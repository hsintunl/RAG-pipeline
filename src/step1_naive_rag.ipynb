{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b454314c-9456-4472-99bc-28d34d2a5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentence_transformers\n",
    "# !pip install \"pymilvus[milvus_lite]\"\n",
    "# !pip install evaluate\n",
    "# !pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f255c2-9352-40ad-b4ed-8e40a5d6ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d98f0af-a31b-4c8d-96db-c5b9da4084f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pymilvus import MilvusClient, FieldSchema, CollectionSchema, DataType\n",
    "import transformers, torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM\n",
    "import ast\n",
    "import evaluate as hf_evaluate\n",
    "from datasets import Dataset\n",
    "\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import pipeline\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b88e40-16ab-4e44-aeef-330cf1a8478e",
   "metadata": {},
   "source": [
    "## Step1: Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd7262ac-a908-4155-a742-3b7e0bfa8116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/connie/Desktop/Fall 2025/LLM/Assignment2/data/processed/rag_mini_wiki.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53921423-caab-4e7d-a6b4-32c647d168cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (3200, 384)\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Encode all passages into vector embeddings\n",
    "embeddings = embedding_model.encode(\n",
    "    df[\"passages\"].tolist(),\n",
    "    batch_size=64,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dea3c18-388e-4f13-b2ec-6bc232bbef47",
   "metadata": {},
   "source": [
    "### Create Milvus Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a629c3f4-7d74-4b93-b239-e8e2e3283887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Milvus client\n",
    "client = MilvusClient(\"rag_wikipedia_mini.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3edce1dd-39b9-47e1-be16-cdc5bea4bc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_milvus_index(client, df, embedding_model, model_name_str):\n",
    "    \"\"\"\n",
    "    Re-encodes data, rebuilds the Milvus collection, and creates the index\n",
    "    for the given embedding model to support Step 4 experimentation.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Rebuilding Milvus Index for {model_name_str} ---\")\n",
    "    \n",
    "    # 1. Encode all passages with the CURRENT embedding model\n",
    "    embeddings = embedding_model.encode(df[\"passages\"].tolist(), batch_size=64)\n",
    "    vector_dim = embeddings.shape[1]\n",
    "    \n",
    "    # 2. Define the Schema using the NEW dimension\n",
    "    id = FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=False)\n",
    "    passage = FieldSchema(name=\"passage\", dtype=DataType.VARCHAR, max_length=3000) \n",
    "    embedding = FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=vector_dim) \n",
    "    schema = CollectionSchema(fields=[id, passage, embedding], description=\"Experiment passages\")\n",
    "\n",
    "    # 3. Drop and Recreate Collection\n",
    "    if client.has_collection(\"rag_mini\"):\n",
    "        client.drop_collection(\"rag_mini\")\n",
    "        \n",
    "    client.create_collection(collection_name=\"rag_mini\", schema=schema, consistency_level=\"Strong\")\n",
    "    \n",
    "    # 4. Prepare and Insert New Data\n",
    "    rag_data = [{\"id\": i, \"passage\": df.iloc[i][\"passages\"], \"embedding\": embeddings[i].tolist()} for i in range(len(df))]\n",
    "    client.insert(collection_name=\"rag_mini\", data=rag_data)\n",
    "    \n",
    "    # 5. Create Index\n",
    "    index_params = MilvusClient.prepare_index_params()\n",
    "    index_params.add_index(field_name=\"embedding\", index_type=\"IVF_FLAT\", metric_type=\"COSINE\", params={\"nlist\": 128})\n",
    "    client.create_index(\"rag_mini\", index_params=index_params)\n",
    "    \n",
    "    # 6. Load Collection\n",
    "    client.load_collection(\"rag_mini\")\n",
    "    \n",
    "    # --- REQUIRED PRINTOUTS ---\n",
    "    print(\"Entity count:\", client.get_collection_stats(\"rag_mini\")[\"row_count\"])\n",
    "    print(\"Collection schema:\", client.describe_collection(\"rag_mini\"))\n",
    "    print(\"Milvus rebuilt and ready for search.\")\n",
    "    \n",
    "    return embedding_model # Return the initialized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1034db1-2226-4d4b-9acc-5e3072ff6195",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODELS = {\n",
    "    \"dim384\": \"all-MiniLM-L6-v2\", \n",
    "    \"dim768\": \"all-mpnet-base-v2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "678bb523-4068-4ee7-8d12-71c74658667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuilding Milvus Index for all-MiniLM-L6-v2 ---\n",
      "Entity count: 3200\n",
      "Collection schema: {'collection_name': 'rag_mini', 'auto_id': False, 'num_shards': 0, 'description': 'Experiment passages', 'fields': [{'field_id': 100, 'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'params': {}, 'is_primary': True}, {'field_id': 101, 'name': 'passage', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 3000}}, {'field_id': 102, 'name': 'embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 384}}], 'functions': [], 'aliases': [], 'collection_id': 0, 'consistency_level': 0, 'properties': {}, 'num_partitions': 0, 'enable_dynamic_field': False}\n",
      "Milvus rebuilt and ready for search.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_MODEL_NAME = EMBEDDING_MODELS['dim384']\n",
    "BASE_EMBEDDING_MODEL = SentenceTransformer(BASE_MODEL_NAME)\n",
    "\n",
    "rebuild_milvus_index(client, df, BASE_EMBEDDING_MODEL, BASE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720e9b51-417b-4bda-8739-872a568211f1",
   "metadata": {},
   "source": [
    "### Prepare QA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4a3d927-5ecb-4cf7-82ca-e8aca4d3398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df = pd.read_csv(\"/Users/connie/Desktop/Fall 2025/LLM/Assignment2/data/processed/rag_mini_wiki_qa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce65d272-c8e5-4a22-81bf-e0ce6aa65a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['test'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e69ba86f-01c4-4da2-8362-b378ce1aa580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'question': 'Was Abraham Lincoln the sixteent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'question': 'Did Lincoln sign the National Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'question': 'Did his mother die of pneumonia?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'question': \"How many long was Lincoln's form...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'question': 'When did Lincoln begin his polit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                test\n",
       "0  {'question': 'Was Abraham Lincoln the sixteent...\n",
       "1  {'question': 'Did Lincoln sign the National Ba...\n",
       "2  {'question': 'Did his mother die of pneumonia?...\n",
       "3  {'question': \"How many long was Lincoln's form...\n",
       "4  {'question': 'When did Lincoln begin his polit..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92145444-b4fc-4f0a-8f3e-b2f5ce4364cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 918 entries, 0 to 917\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   test    918 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 7.3+ KB\n"
     ]
    }
   ],
   "source": [
    "qa_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c7f94a2-331c-4c83-a5a7-1baa128c9bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(qa_df.iloc[0][\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be3aa5b4-2ef0-4dd1-9e2c-47598f19d074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn string back to dict type\n",
    "qa_df[\"test\"] = qa_df[\"test\"].apply(ast.literal_eval)\n",
    "\n",
    "qa_df[\"question\"] = qa_df[\"test\"].apply(lambda x: x[\"question\"])\n",
    "qa_df[\"answer\"] = qa_df[\"test\"].apply(lambda x: x[\"answer\"])\n",
    "qa_df = qa_df.drop(columns=[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ac05d7d-2f17-41cd-a3d4-0cfc20ded67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was Abraham Lincoln the sixteenth President of...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Did Lincoln sign the National Banking Act of 1...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Did his mother die of pneumonia?</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many long was Lincoln's formal education?</td>\n",
       "      <td>18 months</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When did Lincoln begin his political career?</td>\n",
       "      <td>1832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question     answer\n",
       "0  Was Abraham Lincoln the sixteenth President of...        yes\n",
       "1  Did Lincoln sign the National Banking Act of 1...        yes\n",
       "2                   Did his mother die of pneumonia?         no\n",
       "3      How many long was Lincoln's formal education?  18 months\n",
       "4       When did Lincoln begin his political career?       1832"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22d46c88-27fe-43ba-981c-43c049e337d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 918 entries, 0 to 917\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   question  918 non-null    object\n",
      " 1   answer    918 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 14.5+ KB\n"
     ]
    }
   ],
   "source": [
    "qa_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "560fb68e-d65c-42b0-9460-5e5b4c67a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Stratified sampling: 40 short / 40 medium / 40 long ===\n",
    "qa_df[\"q_len\"] = qa_df[\"question\"].apply(lambda x: len(str(x).split()))\n",
    "bins = np.percentile(qa_df[\"q_len\"], [33, 66])\n",
    "qa_df[\"q_bin\"] = pd.cut(\n",
    "    qa_df[\"q_len\"],\n",
    "    bins=[-1, bins[0], bins[1], float(\"inf\")],\n",
    "    labels=[\"short\", \"medium\", \"long\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e86eed9b-035e-4e37-975f-a6ae246238b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k3/vcbwsjp93rl2cww52jf518fc0000gn/T/ipykernel_74297/3120441276.py:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  subset_df = qa_df.groupby(\"q_bin\", group_keys=False).apply(\n",
      "/var/folders/k3/vcbwsjp93rl2cww52jf518fc0000gn/T/ipykernel_74297/3120441276.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  subset_df = qa_df.groupby(\"q_bin\", group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "subset_df = qa_df.groupby(\"q_bin\", group_keys=False).apply(\n",
    "    lambda x: x.sample(min(40, len(x)), random_state=42)\n",
    ").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b0f5a23-5f75-4e03-9580-58900672f6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using 120 stratified queries for ALL columns ---\n",
      "q_bin\n",
      "short     40\n",
      "medium    40\n",
      "long      40\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Using {len(subset_df)} stratified queries for ALL columns ---\")\n",
    "print(subset_df[\"q_bin\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d87de74-5219-41e7-a948-33b0c052c0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>q_len</th>\n",
       "      <th>q_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What did James Monroe make in 1817?</td>\n",
       "      <td>two long tours</td>\n",
       "      <td>7</td>\n",
       "      <td>short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are Gray Wolves native to North America?</td>\n",
       "      <td>Yes</td>\n",
       "      <td>7</td>\n",
       "      <td>short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is English the official language?</td>\n",
       "      <td>yes</td>\n",
       "      <td>5</td>\n",
       "      <td>short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How long is the elephant's gestation period?</td>\n",
       "      <td>22 months</td>\n",
       "      <td>7</td>\n",
       "      <td>short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are diving ducks heavier tha dabbling ducks?</td>\n",
       "      <td>Yes</td>\n",
       "      <td>7</td>\n",
       "      <td>short</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question          answer  q_len  q_bin\n",
       "0           What did James Monroe make in 1817?  two long tours      7  short\n",
       "1      Are Gray Wolves native to North America?             Yes      7  short\n",
       "2             Is English the official language?             yes      5  short\n",
       "3  How long is the elephant's gestation period?       22 months      7  short\n",
       "4  Are diving ducks heavier tha dabbling ducks?             Yes      7  short"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORTANT: From here on, we ONLY use subset_df instead of qa_df\n",
    "qa_df = subset_df  \n",
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4813e0-193d-4528-b9f1-7f626e1de03a",
   "metadata": {},
   "source": [
    "## STEP 2: RETRIEVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb8ec48-ea7f-4a5e-a8d2-8e0094e4c76a",
   "metadata": {},
   "source": [
    "### Single Query Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1195adb0-460a-4820-ae87-533c632b0bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "# Get the first question for a demonstration\n",
    "query = qa_df.iloc[0][\"question\"]\n",
    "\n",
    "# convert query into an embedding vector\n",
    "query_embedding = embedding_model.encode(query)\n",
    "print(query_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c83265f-44af-4718-8922-d2e7bd3a25fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list so it can be passed into Milvus\n",
    "query_embedding_list = query_embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33b05eb6-830e-4bc4-9279-2511eb55cabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: [[{'id': 2520, 'distance': 0.6385066509246826, 'entity': {'id': 2520, 'passage': '{\\'passage\\': \"Monroe had racked up many debts during his years of public life. As a result, he was forced to sell off his Highland Plantation (now called Ash Lawn-Highland; it is owned by his alma mater, the College of William and Mary, which has opened it to the public). Throughout his life, he was not financially solvent, and his wife\\'s poor health made matters worse.  For these reasons, he and his wife lived in Oak Hill until Elizabeth\\'s death on September 23, 1830.\", \\'id\\': 2521}'}}, {'id': 2506, 'distance': 0.6370433568954468, 'entity': {'id': 2506, 'passage': \"{'passage': 'The Presidentâ\\\\x80\\\\x99s parents, father Spence Monroe (ca. 1727 1774), a woodworker and tobacco farmer, and mother Elizabeth Jones Monroe had significant land holdings but little money. Like his parents, he was a slaveholder. Born in Westmoreland County, Virginia, Monroe went to school at Campbelltown Academy and then the College of William and Mary, both in Virginia.  After graduating from William and Mary in 1776, Monroe fought in the Continental Army, serving with distinction at the Battle of Trenton, where he was shot in his left shoulder. He is depicted holding the flag in the famous painting of Washington Crossing the Delaware. Following his military service, he practiced law in Fredericksburg, Virginia. James Monroe married Elizabeth Kortright on February 16, 1786 at the Trinity Church in New York.', 'id': 2507}\"}}, {'id': 2519, 'distance': 0.6267667412757874, 'entity': {'id': 2519, 'passage': '{\\'passage\\': \"When his presidency expired on March 4, 1825, James Monroe lived at Monroe Hill on the grounds of the University of Virginia.  This university\\'s modern campus was Monroe\\'s family farm from 1788 to 1817, but he had sold it in the first year of his presidency to the new college.  He served on the college\\'s Board of Visitors under Jefferson and then under the second rector and another former President James Madison, until his death.\", \\'id\\': 2520}'}}]]\n"
     ]
    }
   ],
   "source": [
    "# Search the database with the query embedding\n",
    "output_ = client.search(\n",
    "    collection_name=\"rag_mini\",\n",
    "    data=[query_embedding_list],\n",
    "    anns_field=\"embedding\",\n",
    "    search_params={\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}},  # search params\n",
    "    limit=3,   # retrieve Top-3 passages\n",
    "    output_fields=[\"id\", \"passage\"]\n",
    ")\n",
    "\n",
    "print(output_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b2a5e6-1f6a-4771-a65d-0e419de4d658",
   "metadata": {},
   "source": [
    "## STEP 3: GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e41ac1f-6a2d-48e7-bf74-39712800bd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant. Use the context to answer the question. Answer with yes or no, then explain briefly. \n",
      " Context: {'passage': \"Monroe had racked up many debts during his years of public life. As a result, he was forced to sell off his Highland Plantation (now called Ash Lawn-Highland; it is owned by his alma mater, the College of William and Mary, which has opened it to the public). Throughout his life, he was not financially solvent, and his wife's poor health made matters worse.  For these reasons, he and his wife lived in Oak Hill until Elizabeth's death on September 23, 1830.\", 'id': 2521}: \n",
      " Question: What did James Monroe make in 1817? \n"
     ]
    }
   ],
   "source": [
    "# Take the top-1 passage as the final context for the LLM\n",
    "context = output_[0][0]['entity']['passage']\n",
    "\n",
    "system_prompt = \"You are a helpful assistant. Use the context to answer the question. Answer with yes or no, then explain briefly.\"\n",
    "\n",
    "# Construct the RAG Prompt\n",
    "prompt = f\"\"\"{system_prompt} \\n Context: {context}: \\n Question: {query} \"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c79d72-2754-4155-92cd-2b80790a8af6",
   "metadata": {},
   "source": [
    "### RAG Response for a Single Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ede861bc-be85-4ebd-bab6-6e66b1375a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: no\n"
     ]
    }
   ],
   "source": [
    "# Load the LLM Model\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "# Generate answer for a single query\n",
    "single_output = generator(prompt, max_new_tokens=100, do_sample=False)\n",
    "\n",
    "# Decode and extract answer\n",
    "answer_single = single_output[0][\"generated_text\"]\n",
    "print(\"Generated Answer:\", answer_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e53ea4-fcbd-41c2-9bb4-ffed83aece88",
   "metadata": {},
   "source": [
    "### BATCH RAG: Retrieve and Generate Responses for all Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e216566f-87ad-4176-8910-46e122da243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prompting Strategies Definitions ---\n",
    "\n",
    "# 1. Naive Prompt (Baseline, similar to your original, but simplified for clarity)\n",
    "PROMPT_NAIVE = \"\"\"\n",
    "You are a helpful assistant. Use the context provided below to answer the question. Only use information found in the context.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# 2. Chain-of-Thought (CoT) Prompt (Forces step-by-step reasoning)\n",
    "# This format ensures the model provides reasoning before the final answer, which is often more accurate.\n",
    "PROMPT_COT = \"\"\"\n",
    "You are an expert analytical assistant.\n",
    "INSTRUCTIONS: First, critically analyze the context and the question. Generate a detailed, step-by-step reasoning process to determine the answer. \n",
    "Finally, provide the concise answer on a new line, labeled \"Final Answer:\". \n",
    "Your full response must include the reasoning steps.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# 3. Persona Prompting (Assigns a specific role to influence tone/focus)\n",
    "# Here, we ask the model to be a \"Concise Historian\" for factual RAG.\n",
    "PROMPT_PERSONA = \"\"\"\n",
    "You are a historical expert known for concise and authoritative answers. \n",
    "Your primary goal is to provide a brief, factual answer based ONLY on the provided context. \n",
    "If the context does not contain the answer, state \"Information not available in the context.\"\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d36d8de-179a-436f-9d7f-bddbb0d7b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experimental configuration ---\n",
    "STRATEGIES_TO_TEST = {\n",
    "    \"naive\": PROMPT_NAIVE,\n",
    "    \"cot\": PROMPT_COT,\n",
    "    \"persona\": PROMPT_PERSONA\n",
    "}\n",
    "\n",
    "RETRIEVAL_K_VALUES = [1, 3, 5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1dfec848-4787-4784-85c1-32de6a34c692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_embedding_experiment(embedding_model, embedding_model_name, qa_df, client, generator):\n",
    "    \"\"\"\n",
    "    Runs the K x Prompting grid search for a single, already indexed embedding model.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Experiment for Embedding Model: {embedding_model_name} ---\")\n",
    "    \n",
    "    qa_df_copy = qa_df.copy() \n",
    "    questions_list = qa_df_copy[\"question\"].tolist()\n",
    "    contexts_storage = {}\n",
    "    \n",
    "    # --- Nested Loops: K x Prompting Strategy ---\n",
    "    for K in RETRIEVAL_K_VALUES:\n",
    "        for strategy_name, system_prompt_template in STRATEGIES_TO_TEST.items():\n",
    "            \n",
    "            print(f\"\\n-> Running K={K}, Strategy={strategy_name} with {embedding_model_name}\")\n",
    "            generated_answers = []\n",
    "            \n",
    "            for i in tqdm(range(len(qa_df_copy)), desc=f\"K={K}, {strategy_name}\"):\n",
    "                q = qa_df_copy.iloc[i][\"question\"]\n",
    "                \n",
    "                # Retrieval Step: Embed the query with the current model\n",
    "                # Uses the model object passed directly: embedding_model.encode(q)\n",
    "                q_emb = embedding_model.encode(q).tolist() # CORRECT\n",
    "                \n",
    "                results = client.search(\n",
    "                    collection_name=\"rag_mini\",\n",
    "                    data=[q_emb],\n",
    "                    anns_field=\"embedding\",\n",
    "                    search_params={\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}}, \n",
    "                    limit=K,  # Use the K value from the outer loop\n",
    "                    output_fields=[\"passage\", \"id\"]\n",
    "                )\n",
    "\n",
    "                # Context Combination\n",
    "                passages = [hit[\"entity\"][\"passage\"] for hit in results[0]]\n",
    "                ctx = \"\\n---\\n\".join(passages)\n",
    "                \n",
    "                # Store K=1 context for the RAGAS baseline (only store the baseline model's K=1 context)\n",
    "                if K == 1 and embedding_model_name == EMBEDDING_MODELS['dim384']:\n",
    "                    contexts_storage[i] = ctx\n",
    "                \n",
    "                # Generation\n",
    "                prompt = system_prompt_template.format(context=ctx, question=q)\n",
    "                out = generator(prompt, max_new_tokens=200, do_sample=False)\n",
    "                ans = out[0][\"generated_text\"]\n",
    "                generated_answers.append(ans)\n",
    "            \n",
    "            # Store results\n",
    "            qa_df_copy[f\"gen_{strategy_name}_k{K}_{embedding_model_name}\"] = generated_answers\n",
    "    \n",
    "    return qa_df_copy, contexts_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5bba043b-78a1-403f-ae4b-f210744b5e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuilding Milvus Index for all-MiniLM-L6-v2 ---\n",
      "Entity count: 3200\n",
      "Collection schema: {'collection_name': 'rag_mini', 'auto_id': False, 'num_shards': 0, 'description': 'Experiment passages', 'fields': [{'field_id': 100, 'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'params': {}, 'is_primary': True}, {'field_id': 101, 'name': 'passage', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 3000}}, {'field_id': 102, 'name': 'embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 384}}], 'functions': [], 'aliases': [], 'collection_id': 0, 'consistency_level': 0, 'properties': {}, 'num_partitions': 0, 'enable_dynamic_field': False}\n",
      "Milvus rebuilt and ready for search.\n",
      "\n",
      "--- Starting Experiment for Embedding Model: all-MiniLM-L6-v2 ---\n",
      "\n",
      "-> Running K=1, Strategy=naive with all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K=1, naive: 100%|█████████████████████████████| 120/120 [00:55<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Running K=1, Strategy=cot with all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K=1, cot: 100%|███████████████████████████████| 120/120 [01:57<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Running K=1, Strategy=persona with all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K=1, persona: 100%|███████████████████████████| 120/120 [00:47<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Running K=3, Strategy=naive with all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K=3, naive:   2%|▌                              | 2/120 [00:01<01:58,  1.00s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n",
      "K=3, naive: 100%|█████████████████████████████| 120/120 [01:22<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Running K=3, Strategy=cot with all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K=3, cot: 100%|███████████████████████████████| 120/120 [02:38<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Running K=3, Strategy=persona with all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K=3, persona: 100%|███████████████████████████| 120/120 [01:33<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Running K=5, Strategy=naive with all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K=5, naive: 100%|█████████████████████████████| 120/120 [02:11<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Running K=5, Strategy=cot with all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K=5, cot: 100%|███████████████████████████████| 120/120 [03:36<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Running K=5, Strategy=persona with all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K=5, persona: 100%|███████████████████████████| 120/120 [01:53<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rebuilding Milvus Index for all-mpnet-base-v2 ---\n",
      "Entity count: 3200\n",
      "Collection schema: {'collection_name': 'rag_mini', 'auto_id': False, 'num_shards': 0, 'description': 'Experiment passages', 'fields': [{'field_id': 100, 'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'params': {}, 'is_primary': True}, {'field_id': 101, 'name': 'passage', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 3000}}, {'field_id': 102, 'name': 'embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 768}}], 'functions': [], 'aliases': [], 'collection_id': 0, 'consistency_level': 0, 'properties': {}, 'num_partitions': 0, 'enable_dynamic_field': False}\n",
      "Milvus rebuilt and ready for search.\n",
      "\n",
      "--- Starting Experiment for Embedding Model: all-mpnet-base-v2 ---\n",
      "\n",
      "-> Running K=1, Strategy=naive with all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K=1, naive: 100%|█████████████████████████████| 120/120 [01:20<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Running K=1, Strategy=cot with all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K=1, cot: 100%|███████████████████████████████| 120/120 [01:48<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Running K=1, Strategy=persona with all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K=1, persona: 100%|███████████████████████████| 120/120 [00:44<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Running K=3, Strategy=naive with all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K=3, naive: 100%|█████████████████████████████| 120/120 [01:37<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Running K=3, Strategy=cot with all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K=3, cot: 100%|███████████████████████████████| 120/120 [02:45<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Running K=3, Strategy=persona with all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K=3, persona: 100%|███████████████████████████| 120/120 [01:10<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Running K=5, Strategy=naive with all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K=5, naive: 100%|█████████████████████████████| 120/120 [01:46<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Running K=5, Strategy=cot with all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K=5, cot: 100%|███████████████████████████████| 120/120 [03:04<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Running K=5, Strategy=persona with all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K=5, persona: 100%|███████████████████████████| 120/120 [02:07<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "final_experiment_results = pd.DataFrame()\n",
    "final_contexts = None\n",
    "\n",
    "for dim_key, model_name in EMBEDDING_MODELS.items():\n",
    "    \n",
    "    # 1. Initialize the specific embedding model\n",
    "    current_embedding_model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # 2. Rebuild the Milvus index for this specific model's vectors\n",
    "    rebuild_milvus_index(client, df, current_embedding_model, model_name)\n",
    "    \n",
    "    # 3. Run the experiment using the model we just indexed with\n",
    "    df_results, contexts_storage = run_embedding_experiment(\n",
    "        current_embedding_model,        # Pass the initialized model\n",
    "        model_name,                     # Pass the model name string\n",
    "        qa_df, \n",
    "        client, \n",
    "        generator\n",
    "    )\n",
    "    \n",
    "    # 4. Merge results (rest of the logic is correct)\n",
    "    if final_experiment_results.empty:\n",
    "        final_experiment_results = df_results\n",
    "        final_contexts = contexts_storage\n",
    "    else:\n",
    "        for col in [c for c in df_results.columns if c.startswith('gen_')]:\n",
    "            final_experiment_results[col] = df_results[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab26eb7-5979-4513-9122-abad4818347b",
   "metadata": {},
   "source": [
    "### Final Data Preparation for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "32161690-de11-483d-8db2-374b58e39a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch generation complete. All experimental results saved.\n"
     ]
    }
   ],
   "source": [
    "final_experiment_results[\"contexts\"] = [final_contexts[i] for i in range(len(qa_df))] \n",
    "\n",
    "# Save the full DataFrame containing ALL experimental results\n",
    "os.makedirs(\"../data/evaluation\", exist_ok=True)\n",
    "final_experiment_results.to_csv(\"../data/evaluation/all_experiment_generations.csv\", index=False)\n",
    "print(\"\\nBatch generation complete. All experimental results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
